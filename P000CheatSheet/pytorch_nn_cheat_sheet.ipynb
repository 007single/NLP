{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Embedding介绍\n",
    "- [https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html)\n",
    "----\n",
    "- pytorch里面实现word embedding是通过一个函数来实现的:nn.Embedding。Embedding的作用就是将词语向量化，通常会将词语表示为一个连续箱梁。\n",
    "- 官方介绍\n",
    "    - 一个简单的查找表，用于存储固定字典和大小的嵌入。\n",
    "    - 此模块通常用于存储单词嵌入并使用索引检索它们。模块的输入是索引列表，输出是相应的字嵌入。\n",
    "    \n",
    "- CLASS torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None)\n",
    "\n",
    "- 参数：\n",
    "    - num_embeddings (int) – 词典的大小，也就是词典有多少个词。比如你有一个30000的词典，这里就是30000.\n",
    "\n",
    "    - embedding_dim (int) – the size of each embedding vector。将词语表示成embedding_dim维的向量。指定向量的维度。\n",
    "\n",
    "    - padding_idx (int, optional) – If given, pads the output with the embedding vector at padding_idx (initialized to zeros) whenever it encounters the index.设置padding_idx后，padding_idx中的嵌入向量将初始化为全零。但是，请注意，之后可以修改该向量，例如，使用定制的初始化方法，从而改变用于填充输出的向量。嵌入中此向量的渐变始终为零。\n",
    "\n",
    "    - max_norm (float, optional) – If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm.\n",
    "    - norm_type (float, optional) – The p of the p-norm to compute for the max_norm option. Default 2.\n",
    "    - scale_grad_by_freq (boolean, optional) – If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False.\n",
    "    - sparse (bool, optional) – If True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.\n",
    "- 看个例子吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0292, -1.2242,  0.1321, -0.2275, -0.3211]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 定义一个词典：word2idx = {'hello': 0, 'world': 1, '!':2}，每个单词我们需要用一个数字去表示它，这里对于hello这个词，用0来表示它。\n",
    "word2idx = {'hello': 0, 'world': 1, '!':2}\n",
    "\n",
    "# 定义Embedding层，这里的3表示词典共有3个词，5表示5维度，其实也就是一个3x5的矩阵.\n",
    "# 如果你有1000个词，每个词希望是100维，你就可以这样建立一个word embedding，nn.Embedding(1000, 100)。\n",
    "# 这就相当于词语和表示词语的向量建立了一张表，想知道一个词的向量表示可以通过这张表去查。\n",
    "embeds = nn.Embedding(3, 5)\n",
    "\n",
    "# 如何查询hello这个词的向量表示呢？\n",
    "\n",
    "# 通过词语在原来字典中的索引查词向量，hello的索引是0\n",
    "hello_idx = torch.LongTensor([word2idx['hello']])\n",
    "\n",
    "# 特别注意这里需要一个Variable，因为我们需要访问nn.Embedding里面定义的元素，且word embeding算是神经网络里面的参数，所以我们需要定义Variable\n",
    "hello_idx = Variable(hello_idx)\n",
    "# 现在输入Variable格式的索引就可以查看词向量了\n",
    "hello_embed = embeds(hello_idx)\n",
    "\n",
    "# 输出hello这个词的<初始词向量>\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意\n",
    "- 注意这里的词向量的建立只是**初始的词向量**，并没有经过任何修改优化，我们需要建立神经网络通过learning修改word embedding里面的参数使得word embedding每一个词向量能够表示每一个不同的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
