{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Log Loss实验\n",
    "- 尝试一个不同的损失函数: binary log loss + 负例采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据文件\n",
    "word_file = './data/bobsue.voc.txt'\n",
    "train_file = './data/bobsue.lm.train.txt'\n",
    "test_file = './data/bobsue.lm.test.txt'\n",
    "dev_file = './data/bobsue.lm.dev.txt'\n",
    "\n",
    "BATCH_SIZE = 32       # 批次大小\n",
    "EMBEDDING_DIM = 200   # 词向量维度\n",
    "EMBEDDING_OUT = 200   # 输出层词向量维度\n",
    "HIDDEN_DIM = 200      # 隐含层\n",
    "GRAD_CLIP = 5.        # 梯度截断值\n",
    "EPOCHS = 20 \n",
    "LEARN_RATE = 0.001    # 初始学习率\n",
    "SAMPLE_NUM = 2        # 负例采样数目\n",
    "\n",
    "BEST_VALID_LOSS = float('inf')     # 初始验证集上的损失值，设为最大\n",
    "MODEL_PATH = \"lm-bll-best-dim{}.pth\"   # 模型名称\n",
    "USE_CUDA = torch.cuda.is_available()    # 是否使用GPU\n",
    "NUM_CUDA = torch.cuda.device_count()    # GPU数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_set(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        word_set = set([line.strip() for line in f])\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_set(*paths):\n",
    "    text = []\n",
    "    for path in paths:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                text.extend(line.split())\n",
    "    word_set = set(text)\n",
    "    word2idx = {w:i for i, w in enumerate(word_set, 1)}\n",
    "    idx2word = {i:w for i, w in enumerate(word_set, 1)}\n",
    "    vocab = Counter(text)\n",
    "    word_counts = torch.tensor([vocab[w] for w in word_set], dtype=torch.float32)\n",
    "    \n",
    "    word_freqs = word_counts / word_counts.sum()\n",
    "    # word_freqs = word_freqs ** (3./4.)\n",
    "    # word_freqs = word_freqs / word_freqs.sum()\n",
    "    return word_set, word2idx, idx2word, word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(filename):\n",
    "    \"\"\"读取数据集，返回句子列表\"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = [line.strip() for line in f]\n",
    "    return sentences\n",
    "\n",
    "def sentences2words(sentences):\n",
    "    return [w for s in sentences for w in s.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set, word2idx, idx2word, word_freqs = create_word_set(train_file, dev_file, test_file)\n",
    "\n",
    "# 设置 <pad> 值为 0\n",
    "PAD_IDX = 0\n",
    "idx2word[PAD_IDX] = '<pad>'\n",
    "word2idx['<pad>'] = PAD_IDX\n",
    "\n",
    "VOCAB_SIZE = len(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1492"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = load_corpus(train_file)\n",
    "dev_sentences = load_corpus(dev_file)\n",
    "test_sentences = load_corpus(test_file)\n",
    "\n",
    "train_words = sentences2words(train_sentences)\n",
    "dev_words = sentences2words(dev_sentences)\n",
    "test_words = sentences2words(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数: 6036，单词数: 71367.\n",
      "验证集句子数: 750，单词数: 8707.\n",
      "测试集句子数: 750，单词数: 8809.\n"
     ]
    }
   ],
   "source": [
    "s = \"{}句子数: {}，单词数: {}.\"\n",
    "print(s.format(\"训练集\", len(train_sentences), len(train_words)))\n",
    "print(s.format(\"验证集\", len(dev_sentences), len(dev_words)))\n",
    "print(s.format(\"测试集\", len(test_sentences), len(test_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sentence_num(sentences):\n",
    "    \"\"\"返回最长句子单词数量\"\"\"\n",
    "    return max([len(s.split()) for s in sentences ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集最长句子单词个数： 21\n",
      "验证集最长句子单词个数： 20\n",
      "测试集最长句子单词个数： 21\n",
      "训练集最短句子单词个数： 5\n",
      "验证集最短句子单词个数： 5\n",
      "测试集最短句子单词个数： 6\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集最长句子单词个数：\", max([len(s.split()) for s in train_sentences ]))\n",
    "print(\"验证集最长句子单词个数：\", max([len(s.split()) for s in dev_sentences ]))\n",
    "print(\"测试集最长句子单词个数：\", max([len(s.split()) for s in test_sentences ]))\n",
    "\n",
    "print(\"训练集最短句子单词个数：\", min([len(s.split()) for s in train_sentences ]))\n",
    "print(\"验证集最短句子单词个数：\", min([len(s.split()) for s in dev_sentences ]))\n",
    "print(\"测试集最短句子单词个数：\", min([len(s.split()) for s in test_sentences ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sequence(corpus, word2idx, word_freqs, sample_num=20, seq_len=21):\n",
    "    \"\"\"输入语料句子列表，返回模型输入序列的idx\"\"\"\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    neg_words = []\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        sentence_tample = [0] * seq_len\n",
    "        for i, w in enumerate(words[:-1]):\n",
    "            sentence_tample[i] = word2idx[w]\n",
    "        target_tample = [0] * seq_len\n",
    "        for i, w in enumerate(words[1:]):\n",
    "            target_tample[i] = word2idx[w]\n",
    "        sentences.append(sentence_tample)\n",
    "        labels.append(target_tample)\n",
    "        # 负例采样\n",
    "        neg_words.append(torch.multinomial(word_freqs, seq_len * sample_num, True))\n",
    "    return (sentences, labels, neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label, train_neg = model_sequence(train_sentences, word2idx, word_freqs, sample_num=SAMPLE_NUM)\n",
    "dev_data, dev_label, dev_neg = model_sequence(dev_sentences, word2idx, word_freqs, sample_num=SAMPLE_NUM)\n",
    "test_data, test_label, test_neg = model_sequence(test_sentences, word2idx, word_freqs, sample_num=SAMPLE_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1433, 1057, 633, 821, 1296, 391, 650, 817, 214, 1080, 1400, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<s> She ate quickly and asked to be taken home . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> "
     ]
    }
   ],
   "source": [
    "a = train_data[0]\n",
    "print(a)\n",
    "for i in a:\n",
    "    print(idx2word[i], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = train_neg[0]\n",
    "n.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_batch_data(data, label, neg, batch_size=32):\n",
    "    \"\"\"\n",
    "    构建 batch tensor，返回 batch 列表，每个batch为三元组包含data和label、neg_word\n",
    "    \"\"\"\n",
    "    batch_data = []\n",
    "    data_tensor = torch.tensor(data, dtype=torch.long)\n",
    "    label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "    neg_tensor = torch.stack(neg)\n",
    "    n, dim = data_tensor.size()\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = start + batch_size\n",
    "        if end > n:\n",
    "            break\n",
    "            dbatch = data_tensor[start: ]\n",
    "            lbatch = label_tensor[start: ]\n",
    "            nbatch = neg_tensor[start: ]\n",
    "            print(\"最后一个batch size:\", dbatch.size())\n",
    "#             break\n",
    "        else:\n",
    "            dbatch = data_tensor[start: end]\n",
    "            lbatch = label_tensor[start: end]\n",
    "            nbatch = neg_tensor[start: end]\n",
    "        batch_data.append((dbatch, lbatch, nbatch))\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = gene_batch_data(train_data, train_label, train_neg, batch_size=BATCH_SIZE)\n",
    "dev_batch = gene_batch_data(dev_data, dev_label, dev_neg, batch_size=BATCH_SIZE)\n",
    "test_batch = gene_batch_data(test_data, test_label, test_neg, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNegModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, embedding_out, hidden_dim, vocab_size, sample_num):\n",
    "        super(LSTMNegModel, self).__init__()\n",
    "        self.sample_num = sample_num\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embedding_out)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, embedding_out)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        text, label, neg = data\n",
    "        mask = (text != PAD_IDX)\n",
    "        neg_mask = mask.unsqueeze(1).expand(text.size(0), SAMPLE_NUM, text.size(1)).contiguous().view(neg.size(0), neg.size(1))\n",
    "#         print(mask.size(), neg_mask.size())\n",
    "        # [batch, seq_len] -> [batch, seq_len, emb_dim]\n",
    "        embed = self.in_embed(text)\n",
    "        label_embed = self.out_embed(label)\n",
    "        # [batch, seq_len*sample_num] -> [batch, seq_len*sample_num, emb_dim]\n",
    "        neg_embed = self.out_embed(neg)\n",
    "        \n",
    "        # [batch, seq_len, emb_dim] -> [batch, seq_len, hidden_size]\n",
    "        out, _ = self.lstm(embed)\n",
    "        # [batch, seq_len, hidden_size] -> [batch, seq_len, emb_dim]\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        # 计算损失\n",
    "        # [batch, seq_len, emb_dim] * [batch, seq_len, emb_dim] -> [batch, seq_len]\n",
    "        label_score = (out * label_embed).sum(2)\n",
    "#         label_score = torch.mm(label_embed.squeeze(1), out.squeeze(1).permute(1, 0))\n",
    "        # [batch, seq_len*sample_num, emb_dim] * [batch, seq_len*sample_num, emb_dim] \n",
    "        out_expand = out.unsqueeze(1).expand(out.size(0), SAMPLE_NUM, out.size(1), out.size(2)).contiguous().view(neg_embed.size(0),neg_embed.size(1),neg_embed.size(2),)\n",
    "        # [batch, seq_len*sample_num, emb_dim] -> [batch, seq_len*sample_num]\n",
    "        neg_score = (out_expand * neg_embed).sum(2)\n",
    "#         neg_score = torch.mm(neg_embed.view(-1, self.embedding_dim), neg_out)\n",
    "\n",
    "        label_score = label_score[mask]\n",
    "        neg_score = neg_score[neg_mask]\n",
    "\n",
    "        log_label = F.logsigmoid(label_score).mean()\n",
    "        log_neg = torch.log(1 - torch.sigmoid(neg_score)).mean()\n",
    "\n",
    "        loss = log_label + log_neg\n",
    "        \n",
    "        return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2idx)\n",
    "model = LSTMNegModel(EMBEDDING_DIM, EMBEDDING_OUT, HIDDEN_DIM, VOCAB_SIZE, SAMPLE_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device(\"cuda\" if USE_CUDA else 'cpu')\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "model = model.to(DEVICE)\n",
    "# if NUM_CUDA > 1:\n",
    "#     device_ids = list(range(NUM_CUDA))\n",
    "#     print(device_ids)\n",
    "#     model = nn.DataParallel(model, device_ids=device_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, iterator):\n",
    "    model.eval()  # 不更新参数，预测模式\n",
    "    epoch_loss=0  # 积累变量\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y, z in iterator:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            z = z.to(device)\n",
    "            \n",
    "            loss = model((x,y,z))\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "\n",
    "def train(model, device, iterator, optimizer, grad_clip):\n",
    "    epoch_loss = 0  # 积累变量\n",
    "    model.train()   # 该函数表示PHASE=Train\n",
    "    \n",
    "    for x, y, z in iterator:  # 拿每一个minibatch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        z = z.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        loss = model((x,y,z))  # loss\n",
    "        loss.backward()        # 进行BP\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()  # 更新参数\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current lr: 0.001\n",
      "Epoch:1|Train Loss:nan|Val Loss:nan\n",
      "Current lr: 0.0005\n",
      "Epoch:2|Train Loss:nan|Val Loss:nan\n",
      "Epoch:3|Train Loss:nan|Val Loss:nan\n",
      "Epoch:4|Train Loss:nan|Val Loss:nan\n",
      "Epoch:5|Train Loss:nan|Val Loss:nan\n",
      "Epoch:6|Train Loss:nan|Val Loss:nan\n",
      "Early stop!\n"
     ]
    }
   ],
   "source": [
    "# criterion = nn.NLLLoss()            # 指定损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARN_RATE)  # 指定优化器\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)   # 学习率缩减？\n",
    "\n",
    "SCHED_NUM = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss = train(model, DEVICE, train_batch, optimizer, GRAD_CLIP)\n",
    "    valid_loss = evaluate(model, DEVICE, dev_batch)\n",
    "    if valid_loss < BEST_VALID_LOSS: # 如果是最好的模型就保存到文件夹\n",
    "        BEST_VALID_LOSS = valid_loss\n",
    "        torch.save(model, MODEL_PATH.format(EMBEDDING_DIM))\n",
    "        SCHED_NUM = 0\n",
    "    else:\n",
    "        SCHED_NUM += 1\n",
    "        if SCHED_NUM // 3 == 0:\n",
    "            scheduler.step()\n",
    "            print(\"Current lr:\", optimizer.param_groups[0]['lr'])\n",
    "        if SCHED_NUM == 7:\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "    print('Epoch:{0}|Train Loss:{1}|Val Loss:{2}'.format(epoch,train_loss,valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
