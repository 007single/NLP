{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Log Loss实验\n",
    "- 尝试一个不同的损失函数: binary log loss + 负例采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据文件\n",
    "word_file = './data/bobsue.voc.txt'\n",
    "train_file = './data/bobsue.lm.train.txt'\n",
    "test_file = './data/bobsue.lm.test.txt'\n",
    "dev_file = './data/bobsue.lm.dev.txt'\n",
    "\n",
    "BATCH_SIZE = 32       # 批次大小\n",
    "EMBEDDING_DIM = 200   # 词向量维度\n",
    "EMBEDDING_OUT = 100   # 输出层词向量维度\n",
    "HIDDEN_DIM = 200      # 隐含层\n",
    "GRAD_CLIP = 5.        # 梯度截断值\n",
    "EPOCHS = 20\n",
    "LEARN_RATE = 0.001    # 初始学习率\n",
    "SAMPLE_NUM = 10       # 负例采样数目\n",
    "\n",
    "BEST_VALID_LOSS = float('inf')          # 初始验证集上的损失值，设为最大\n",
    "MODEL_PATH = \"lm-bll-samp-{}.pth\"       # 模型名称\n",
    "USE_CUDA = torch.cuda.is_available()    # 是否使用GPU\n",
    "NUM_CUDA = torch.cuda.device_count()    # GPU数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_set(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        word_set = set([line.strip() for line in f])\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_set(*paths, power=1):\n",
    "    text = []\n",
    "    for path in paths:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                text.extend(line.split())\n",
    "    word_set = set(text)\n",
    "    word2idx = {w:i for i, w in enumerate(word_set, 1)}\n",
    "    idx2word = {i:w for i, w in enumerate(word_set, 1)}\n",
    "    vocab = Counter(text)\n",
    "    word_counts = torch.tensor([vocab[w] for w in word_set], dtype=torch.float32)\n",
    "    \n",
    "    word_freqs = word_counts / word_counts.sum()\n",
    "    word_freqs = word_freqs ** power\n",
    "    word_freqs = word_freqs / word_freqs.sum()\n",
    "    return word_set, word2idx, idx2word, word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(filename):\n",
    "    \"\"\"读取数据集，返回句子列表\"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = [line.strip() for line in f]\n",
    "    return sentences\n",
    "\n",
    "def sentences2words(sentences):\n",
    "    return [w for s in sentences for w in s.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set, word2idx, idx2word, word_freqs = create_word_set(train_file, dev_file, test_file, power=1)\n",
    "\n",
    "# 设置 <pad> 值为 0\n",
    "PAD_IDX = 0\n",
    "idx2word[PAD_IDX] = '<pad>'\n",
    "word2idx['<pad>'] = PAD_IDX\n",
    "\n",
    "VOCAB_SIZE = len(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1492"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = load_corpus(train_file)\n",
    "dev_sentences = load_corpus(dev_file)\n",
    "test_sentences = load_corpus(test_file)\n",
    "\n",
    "train_words = sentences2words(train_sentences)\n",
    "dev_words = sentences2words(dev_sentences)\n",
    "test_words = sentences2words(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数: 6036，单词数: 71367.\n",
      "验证集句子数: 750，单词数: 8707.\n",
      "测试集句子数: 750，单词数: 8809.\n"
     ]
    }
   ],
   "source": [
    "s = \"{}句子数: {}，单词数: {}.\"\n",
    "print(s.format(\"训练集\", len(train_sentences), len(train_words)))\n",
    "print(s.format(\"验证集\", len(dev_sentences), len(dev_words)))\n",
    "print(s.format(\"测试集\", len(test_sentences), len(test_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sentence_num(sentences):\n",
    "    \"\"\"返回最长句子单词数量\"\"\"\n",
    "    return max([len(s.split()) for s in sentences ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集最长句子单词个数： 21\n",
      "验证集最长句子单词个数： 20\n",
      "测试集最长句子单词个数： 21\n",
      "训练集最短句子单词个数： 5\n",
      "验证集最短句子单词个数： 5\n",
      "测试集最短句子单词个数： 6\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集最长句子单词个数：\", max([len(s.split()) for s in train_sentences ]))\n",
    "print(\"验证集最长句子单词个数：\", max([len(s.split()) for s in dev_sentences ]))\n",
    "print(\"测试集最长句子单词个数：\", max([len(s.split()) for s in test_sentences ]))\n",
    "\n",
    "print(\"训练集最短句子单词个数：\", min([len(s.split()) for s in train_sentences ]))\n",
    "print(\"验证集最短句子单词个数：\", min([len(s.split()) for s in dev_sentences ]))\n",
    "print(\"测试集最短句子单词个数：\", min([len(s.split()) for s in test_sentences ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sequence(corpus, word2idx, word_freqs, sample_num=20, seq_len=21):\n",
    "    \"\"\"输入语料句子列表，返回模型输入序列的idx\"\"\"\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    neg_words = []\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        sentence_tample = [0] * seq_len\n",
    "        for i, w in enumerate(words[:-1]):\n",
    "            sentence_tample[i] = word2idx[w]\n",
    "        target_tample = [0] * seq_len\n",
    "        for i, w in enumerate(words[1:]):\n",
    "            target_tample[i] = word2idx[w]\n",
    "        sentences.append(sentence_tample)\n",
    "        labels.append(target_tample)\n",
    "        # 负例采样\n",
    "        neg_words.append(torch.multinomial(word_freqs, seq_len * sample_num, True))\n",
    "    return (sentences, labels, neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label, train_neg = model_sequence(train_sentences, word2idx, word_freqs, sample_num=SAMPLE_NUM)\n",
    "dev_data, dev_label, dev_neg = model_sequence(dev_sentences, word2idx, word_freqs, sample_num=SAMPLE_NUM)\n",
    "test_data, test_label, test_neg = model_sequence(test_sentences, word2idx, word_freqs, sample_num=SAMPLE_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 725, 490, 79, 638, 1358, 721, 328, 729, 665, 1442, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<s> She ate quickly and asked to be taken home . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> ----------------------------------------\n",
      "She ate quickly and asked to be taken home . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> ----------------------------------------\n",
      "tensor([ 673, 1441, 1441,  844,    8, 1326,   68,   26,  863, 1049,  248,   43,\n",
      "         938,  247,    8,  325,  299,    8,   26, 1006,   26, 1441,  568,  637,\n",
      "        1065,  247,    8,  828, 1441, 1120,  782,  828,  828,  747,  735,  808,\n",
      "         253,  465,  559, 1441,   26, 1195,  568, 1441,  180,  701,  530, 1086,\n",
      "         289, 1169,   75, 1314,  851, 1441, 1441,  273,  497, 1443,  102, 1441,\n",
      "          75,   75, 1156,  380, 1121, 1441,  822,  116, 1202,   26, 1202,  688,\n",
      "         938,  863,   26,  720, 1154, 1370,  720,  912, 1377,   26,  794,  893,\n",
      "        1472,  555, 1413,  811,  828,    8,  720,  637,  664, 1202,  147, 1190,\n",
      "         822,   68, 1111,  380,    8,  836,    8,    8,  724,  844,   75,    8,\n",
      "         571,   26,  411,  863,  999,   30,  524,   75,   73,  957,   75,  720,\n",
      "          68, 1441,    8,  284,   26,   75,  195, 1077, 1418, 1145, 1441,  527,\n",
      "           8,    8,  555,  724,  530,  420,  283, 1472,  938,  938,    8,  703,\n",
      "          75,  724,  637, 1150,   26, 1441,    8, 1416,  312,  380,  978, 1202,\n",
      "        1328,  720,  539, 1441,  938,  248,  912, 1443,   62,  637, 1441,  316,\n",
      "        1441,   26,  996,  240,    8,  462, 1341,   68,  874,  938, 1290, 1460,\n",
      "        1482,   26,  745, 1321, 1202,  938, 1103,  234, 1235,  288,  828,   26,\n",
      "         578,    8,   26,  322, 1441, 1016, 1441,  111, 1441,   26, 1202,  720,\n",
      "          26,  544,  742,  411,  511,  743])\n"
     ]
    }
   ],
   "source": [
    "a = train_data[0]\n",
    "print(a)\n",
    "for i in a:\n",
    "    print(idx2word[i], end=' ')\n",
    "print(\"--\"*20)\n",
    "b = train_label[0]\n",
    "for i in b:\n",
    "    print(idx2word[i], end=' ')\n",
    "print(\"--\"*20)\n",
    "print(train_neg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([210])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = train_neg[0]\n",
    "n.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_batch_data(data, label, neg, batch_size=32):\n",
    "    \"\"\"\n",
    "    构建 batch tensor，返回 batch 列表，每个batch为三元组包含data和label、neg_word\n",
    "    \"\"\"\n",
    "    batch_data = []\n",
    "    data_tensor = torch.tensor(data, dtype=torch.long)\n",
    "    label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "    neg_tensor = torch.stack(neg)\n",
    "    n, dim = data_tensor.size()\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = start + batch_size\n",
    "        if end > n:\n",
    "            break\n",
    "            dbatch = data_tensor[start: ]\n",
    "            lbatch = label_tensor[start: ]\n",
    "            nbatch = neg_tensor[start: ]\n",
    "            print(\"最后一个batch size:\", dbatch.size())\n",
    "#             break\n",
    "        else:\n",
    "            dbatch = data_tensor[start: end]\n",
    "            lbatch = label_tensor[start: end]\n",
    "            nbatch = neg_tensor[start: end]\n",
    "        batch_data.append((dbatch, lbatch, nbatch))\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = gene_batch_data(train_data, train_label, train_neg, batch_size=BATCH_SIZE)\n",
    "dev_batch = gene_batch_data(dev_data, dev_label, dev_neg, batch_size=BATCH_SIZE)\n",
    "test_batch = gene_batch_data(test_data, test_label, test_neg, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNegModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, embedding_out, hidden_dim, vocab_size, sample_num):\n",
    "        super(LSTMNegModel, self).__init__()\n",
    "        self.sample_num = sample_num\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embedding_out)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, embedding_out)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        text, label, neg = data\n",
    "        # print(\"-\"*20)\n",
    "        # print(text.size())\n",
    "        # print(label.size())\n",
    "        # print(neg.size())   # (bacth, SAMPLE_NUM*seq_len)\n",
    "        # (torch.tensor([1,2,3,1]) != 1) ==>[0,1,1, 0]\n",
    "        mask = (text != PAD_IDX)     # (batch, seq_len)\n",
    "        # print(\"mask:\", mask.size())\n",
    "        # (batch, seq_len)-->(batch, 1, seq_len)-->(batch,SAMPLE_NUM,seq_len)-->（batch, SAMPLE_NUM*seq_len)\n",
    "        neg_mask = mask.unsqueeze(1).expand(text.size(0), SAMPLE_NUM, text.size(1)).contiguous().view(neg.size(0), neg.size(1))\n",
    "        # 当调用contiguous()时，会强制拷贝一份tensor\n",
    "\n",
    "        # print(\"neg_mask:\", neg_mask.size(), neg_mask.sum())  # (batch, seq_len*sample_num)\n",
    "        \n",
    "        embed = self.in_embed(text)   # (bacth,seq_len) --> (bacth, seq_len, in_emd_dim)\n",
    "        \n",
    "        # (batch, seq_len) -> (batch, seq_len, out_emb_dim)\n",
    "        label_embed = self.out_embed(label)\n",
    "        # (batch, seq_len*sample_num)-> (batch, seq_len*sample_num, out_emb_dim)\n",
    "        neg_embed = self.out_embed(neg)\n",
    "        \n",
    "        # (batch, seq_len, in_emb_dim) -> (batch, seq_len, out_emb_dim(hn_dim))\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embed)\n",
    "        # (batch, seq_len, out_emb_dim) -> (batch, seq_len, out_emb_dim) 即形状不变\n",
    "        out = self.linear(lstm_out)\n",
    "        \n",
    "        # 计算损失\n",
    "        # (batch, seq_len, out_emb_dim) * (batch, seq_len, out_emb_dim) -> sum(2)-(batch, seq_len)\n",
    "        # 对应元素相乘，2维度上求和\n",
    "        label_score = (out * label_embed).sum(2)\n",
    "        # label_score = torch.mm(label_embed.squeeze(1), out.squeeze(1).permute(1, 0))\n",
    "        # (batch, seq_len*sample_num, out_emb_dim) * (batch, seq_len*sample_num, out_emb_dim) \n",
    "        out_expand = out.unsqueeze(1).expand(out.size(0), SAMPLE_NUM, out.size(1), \n",
    "                                             out.size(2)).contiguous().view(\n",
    "                                             neg_embed.size(0), neg_embed.size(1), neg_embed.size(2))\n",
    "        # (batch, seq_len*sample_num, out_emb_dim) -> (batch, seq_len*sample_num)\n",
    "        # 词向量合成一个数的意义是什么？\n",
    "        neg_score = (out_expand * neg_embed).sum(2)\n",
    "\n",
    "        label_score = label_score[mask]    # 这个操作会压缩成一行\n",
    "        neg_score = neg_score[neg_mask]\n",
    "        \n",
    "        log_label = F.logsigmoid(label_score).mean()   # 一个常数 \n",
    "        log_neg = torch.log(1 - torch.sigmoid(neg_score)).mean()\n",
    "\n",
    "        loss = log_label + log_neg\n",
    "        \n",
    "        return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2idx)\n",
    "model = LSTMNegModel(EMBEDDING_DIM, EMBEDDING_OUT, HIDDEN_DIM, VOCAB_SIZE, SAMPLE_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device(\"cuda\" if USE_CUDA else 'cpu')\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "model = model.to(DEVICE)\n",
    "# if NUM_CUDA > 1:\n",
    "#     device_ids = list(range(NUM_CUDA))\n",
    "#     print(device_ids)\n",
    "#     model = nn.DataParallel(model, device_ids=device_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_score(y_hat, y):\n",
    "    # 返回最大的概率的索引\n",
    "    pred = y_hat.argmax(dim=1)\n",
    "    # print(y.view(-1))\n",
    "    acc_count = torch.eq(pred, y.view(-1))\n",
    "    score = acc_count.sum().item() / acc_count.size()[0]\n",
    "    return score\n",
    "\n",
    "def evaluate(model, device, iterator):\n",
    "    epoch_loss = 0  # 积累变量\n",
    "    model.eval()  # 不更新参数，预测模式\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y, z in iterator:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            z = z.to(device)\n",
    "            \n",
    "            loss = model((x,y,z))\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "\n",
    "def train(model, device, iterator, optimizer, grad_clip):\n",
    "    epoch_loss = 0  # 积累变量\n",
    "    model.train()   # 该函数表示PHASE=Train\n",
    "    \n",
    "    for x, y, z in iterator:  # 拿每一个minibatch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        z = z.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        loss = model((x,y,z))  # loss\n",
    "        loss.backward()        # 进行BP\n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()  # 更新参数\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type LSTMNegModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model path:lm-bll-samp-10.pth| train loss 0.6765867387677761| valid loss 0.49544699554858\n",
      "Epoch:1|Train Loss:0.6765867387677761|Val Loss:0.49544699554858\n",
      "Save model path:lm-bll-samp-10.pth| train loss 0.40142372963910405| valid loss 0.40998030875040137\n",
      "Epoch:2|Train Loss:0.40142372963910405|Val Loss:0.40998030875040137\n",
      "Save model path:lm-bll-samp-10.pth| train loss 0.31070439731504057| valid loss 0.38939876919207367\n",
      "Epoch:3|Train Loss:0.31070439731504057|Val Loss:0.38939876919207367\n",
      "Epoch:4|Train Loss:0.24797511615968765|Val Loss:0.3964136398356894\n",
      "Epoch:5|Train Loss:0.1953815960503639|Val Loss:0.427851562914641\n",
      "Current lr: 0.001\n",
      "Epoch:6|Train Loss:0.15279908184992505|Val Loss:0.4718990014946979\n",
      "Epoch:7|Train Loss:0.12087223809608753|Val Loss:0.539731247269589\n",
      "Epoch:8|Train Loss:0.09807310455498543|Val Loss:0.6311629868072012\n",
      "Current lr: 0.0005\n",
      "Epoch:9|Train Loss:0.08436341861144026|Val Loss:inf\n",
      "Early stop!\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARN_RATE)  # 指定优化器\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)   # 学习率缩减？\n",
    "\n",
    "SCHED_NUM = 0\n",
    "model_name = MODEL_PATH.format(SAMPLE_NUM)\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss = train(model, DEVICE, train_batch, optimizer, GRAD_CLIP)\n",
    "    valid_loss = evaluate(model, DEVICE, dev_batch)\n",
    "    if valid_loss < BEST_VALID_LOSS: # 如果是最好的模型就保存到文件夹\n",
    "        BEST_VALID_LOSS = valid_loss\n",
    "        torch.save(model, model_name)\n",
    "        print(\"Save model path:{}| train loss {}| valid loss {}\".format(model_name, train_loss, valid_loss))\n",
    "        SCHED_NUM = 0\n",
    "    else:\n",
    "        SCHED_NUM += 1\n",
    "        if SCHED_NUM % 3 == 0:\n",
    "            scheduler.step()\n",
    "            print(\"Current lr:\", optimizer.param_groups[0]['lr'])\n",
    "        if SCHED_NUM == 7:\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "    print('Epoch:{}|Train Loss:{}|Val Loss:{}'.format(epoch, train_loss, valid_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4409688711166382\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(model_name)\n",
    "test_loss = evaluate(model, DEVICE, test_batch)\n",
    "print('Test Loss: {}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题\n",
    "- 在使用binary log loss 的情况下，如何评价模型？\n",
    "- 梯度截断的情况下依然会存在loss nan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同负采样数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***负采样数量20***\n",
      "Save model path:lm-bll-samp-20.pth| train loss 0.6862944694275551| valid loss 0.499319939509682\n",
      "Epoch:1|Train Loss:0.6862944694275551|Val Loss:0.499319939509682\n",
      "Save model path:lm-bll-samp-20.pth| train loss 0.4045477717163715| valid loss 0.4103606472844663\n",
      "Epoch:2|Train Loss:0.4045477717163715|Val Loss:0.4103606472844663\n",
      "Save model path:lm-bll-samp-20.pth| train loss 0.31166522775558714| valid loss 0.38945959314056067\n",
      "Epoch:3|Train Loss:0.31166522775558714|Val Loss:0.38945959314056067\n",
      "Epoch:4|Train Loss:0.248858502253573|Val Loss:0.39644569288129394\n",
      "Epoch:5|Train Loss:0.1974467286404143|Val Loss:0.43040112179258594\n",
      "Current lr: 0.001\n",
      "Epoch:6|Train Loss:0.15638761119322575|Val Loss:0.48479544209397357\n",
      "Epoch:7|Train Loss:0.12547429262640628|Val Loss:0.5508403739203578\n",
      "Epoch:8|Train Loss:0.10333875808468525|Val Loss:inf\n",
      "Current lr: 0.0005\n",
      "Epoch:9|Train Loss:0.08819240823071053|Val Loss:inf\n",
      "Early stop!\n",
      "Start test model: lm-bll-samp-20.pth\n",
      "Test Loss: 0.4002216497193212\n",
      "***负采样数量100***\n",
      "Save model path:lm-bll-samp-100.pth| train loss 0.6720305584529613| valid loss 0.47874922596889996\n",
      "Epoch:1|Train Loss:0.6720305584529613|Val Loss:0.47874922596889996\n",
      "Save model path:lm-bll-samp-100.pth| train loss 0.39481705506431297| valid loss 0.4017593109089395\n",
      "Epoch:2|Train Loss:0.39481705506431297|Val Loss:0.4017593109089395\n",
      "Save model path:lm-bll-samp-100.pth| train loss 0.30808551387583955| valid loss 0.3841300218001656\n",
      "Epoch:3|Train Loss:0.30808551387583955|Val Loss:0.3841300218001656\n",
      "Epoch:4|Train Loss:0.24812766782780912|Val Loss:0.3909660085387852\n",
      "Epoch:5|Train Loss:0.19934658650705156|Val Loss:0.42308238019113953\n",
      "Current lr: 0.001\n",
      "Epoch:6|Train Loss:0.16092979828728007|Val Loss:0.4746950152127639\n",
      "Epoch:7|Train Loss:0.13272394231976348|Val Loss:0.5496890298698259\n",
      "Epoch:8|Train Loss:nan|Val Loss:nan\n",
      "Current lr: 0.0005\n",
      "Epoch:9|Train Loss:nan|Val Loss:nan\n",
      "Early stop!\n",
      "Start test model: lm-bll-samp-100.pth\n",
      "Test Loss: 0.39808937648068304\n",
      "***负采样数量500***\n",
      "Save model path:lm-bll-samp-500.pth| train loss 0.6614054969333588| valid loss 0.4709318228389906\n",
      "Epoch:1|Train Loss:0.6614054969333588|Val Loss:0.4709318228389906\n",
      "Save model path:lm-bll-samp-500.pth| train loss 0.3833058568391394| valid loss 0.3941392963347228\n",
      "Epoch:2|Train Loss:0.3833058568391394|Val Loss:0.3941392963347228\n",
      "Save model path:lm-bll-samp-500.pth| train loss 0.2991236074966319| valid loss 0.3755582208218782\n",
      "Epoch:3|Train Loss:0.2991236074966319|Val Loss:0.3755582208218782\n",
      "Epoch:4|Train Loss:0.24023729317048761|Val Loss:0.3866316326286482\n",
      "Epoch:5|Train Loss:0.1926637834056895|Val Loss:0.4283683934937353\n",
      "Current lr: 0.001\n",
      "Epoch:6|Train Loss:nan|Val Loss:nan\n",
      "Epoch:7|Train Loss:nan|Val Loss:nan\n",
      "Epoch:8|Train Loss:nan|Val Loss:nan\n",
      "Current lr: 0.0005\n",
      "Epoch:9|Train Loss:nan|Val Loss:nan\n",
      "Early stop!\n",
      "Start test model: lm-bll-samp-500.pth\n",
      "Test Loss: 0.39041490528894507\n"
     ]
    }
   ],
   "source": [
    "sample_num = [20, 100, 500]\n",
    "for n in sample_num:\n",
    "    print(\"***负采样数量{}***\".format(n))\n",
    "    model_name = 'lm-bll-samp-{}.pth'.format(n)\n",
    "    SAMPLE_NUM = n\n",
    "    BEST_VALID_LOSS = float('inf')\n",
    "    train_data, train_label, train_neg = model_sequence(train_sentences, word2idx, word_freqs, sample_num=SAMPLE_NUM)\n",
    "    dev_data, dev_label, dev_neg = model_sequence(dev_sentences, word2idx, word_freqs, sample_num=SAMPLE_NUM)\n",
    "    test_data, test_label, test_neg = model_sequence(test_sentences, word2idx, word_freqs, sample_num=SAMPLE_NUM)\n",
    "    \n",
    "    \n",
    "    train_batch = gene_batch_data(train_data, train_label, train_neg, batch_size=BATCH_SIZE)\n",
    "    dev_batch = gene_batch_data(dev_data, dev_label, dev_neg, batch_size=BATCH_SIZE)\n",
    "    test_batch = gene_batch_data(test_data, test_label, test_neg, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    model = LSTMNegModel(EMBEDDING_DIM, EMBEDDING_OUT, HIDDEN_DIM, VOCAB_SIZE, SAMPLE_NUM)\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARN_RATE)  # 指定优化器\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)   # 学习率缩减？\n",
    "\n",
    "    SCHED_NUM = 0\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        train_loss = train(model, DEVICE, train_batch, optimizer, GRAD_CLIP)\n",
    "        valid_loss = evaluate(model, DEVICE, dev_batch)\n",
    "        if valid_loss < BEST_VALID_LOSS: # 如果是最好的模型就保存到文件夹\n",
    "            BEST_VALID_LOSS = valid_loss\n",
    "            torch.save(model, model_name)\n",
    "            print(\"Save model path:{}| train loss {}| valid loss {}\".format(model_name, train_loss, valid_loss))\n",
    "            SCHED_NUM = 0\n",
    "        else:\n",
    "            SCHED_NUM += 1\n",
    "            if SCHED_NUM % 3 == 0:\n",
    "                scheduler.step()\n",
    "                print(\"Current lr:\", optimizer.param_groups[0]['lr'])\n",
    "            if SCHED_NUM == 7:\n",
    "                print(\"Early stop!\")\n",
    "                break\n",
    "        print('Epoch:{}|Train Loss:{}|Val Loss:{}'.format(epoch, train_loss, valid_loss))\n",
    "    print(\"Start test model:\", model_name)\n",
    "    model = torch.load(model_name)\n",
    "    test_loss = evaluate(model, DEVICE, test_batch)\n",
    "    print('Test Loss: {}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同采样频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***负采样评率0.25***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type LSTMNegModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model path:lm-bll-power-25.0.pth| train loss 0.9979828307603268| valid loss 0.7766682531522668\n",
      "Epoch:1|Train Loss:0.9979828307603268|Val Loss:0.7766682531522668\n",
      "Save model path:lm-bll-power-25.0.pth| train loss 0.6399219172432068| valid loss 0.6289420879405477\n",
      "Epoch:2|Train Loss:0.6399219172432068|Val Loss:0.6289420879405477\n",
      "Save model path:lm-bll-power-25.0.pth| train loss 0.5007797333788364| valid loss 0.5997734380804974\n",
      "Epoch:3|Train Loss:0.5007797333788364|Val Loss:0.5997734380804974\n",
      "Epoch:4|Train Loss:0.4047311301878158|Val Loss:0.6082124178824218\n",
      "Epoch:5|Train Loss:0.32498398534160977|Val Loss:0.6545262673626775\n",
      "Current lr: 0.001\n",
      "Epoch:6|Train Loss:0.2612926849817976|Val Loss:0.72973189405773\n",
      "Epoch:7|Train Loss:0.21327265241044632|Val Loss:0.8235294067341349\n",
      "Epoch:8|Train Loss:0.1791047714650631|Val Loss:0.8798693444417871\n",
      "Current lr: 0.0005\n",
      "Epoch:9|Train Loss:0.15849060355190267|Val Loss:0.9844991538835608\n",
      "Early stop!\n",
      "Start test model: lm-bll-power-25.0.pth\n",
      "Test Loss: 0.6105088228764741\n",
      "***负采样评率0.5***\n",
      "Save model path:lm-bll-power-50.0.pth| train loss 0.9324578931356998| valid loss 0.7056591173876887\n",
      "Epoch:1|Train Loss:0.9324578931356998|Val Loss:0.7056591173876887\n",
      "Save model path:lm-bll-power-50.0.pth| train loss 0.5866700864218651| valid loss 0.5856103767519412\n",
      "Epoch:2|Train Loss:0.5866700864218651|Val Loss:0.5856103767519412\n",
      "Save model path:lm-bll-power-50.0.pth| train loss 0.46142687616830175| valid loss 0.5631061092666958\n",
      "Epoch:3|Train Loss:0.46142687616830175|Val Loss:0.5631061092666958\n",
      "Epoch:4|Train Loss:0.3731820180695108|Val Loss:0.5789777429207511\n",
      "Epoch:5|Train Loss:0.29953039659464614|Val Loss:0.6285732181175895\n",
      "Current lr: 0.001\n",
      "Epoch:6|Train Loss:0.24056981028394497|Val Loss:0.7036158390667128\n",
      "Epoch:7|Train Loss:0.1967496504967517|Val Loss:0.7741440975147745\n",
      "Epoch:8|Train Loss:0.16512219845614534|Val Loss:0.8199642559756404\n",
      "Current lr: 0.0005\n",
      "Epoch:9|Train Loss:0.1453156581029613|Val Loss:inf\n",
      "Early stop!\n",
      "Start test model: lm-bll-power-50.0.pth\n",
      "Test Loss: 0.5811127476070238\n",
      "***负采样评率0.75***\n",
      "Save model path:lm-bll-power-75.0.pth| train loss 0.8152193899484391| valid loss 0.614263073257778\n",
      "Epoch:1|Train Loss:0.8152193899484391|Val Loss:0.614263073257778\n",
      "Save model path:lm-bll-power-75.0.pth| train loss 0.5104170639781241| valid loss 0.5084582035956176\n",
      "Epoch:2|Train Loss:0.5104170639781241|Val Loss:0.5084582035956176\n",
      "Save model path:lm-bll-power-75.0.pth| train loss 0.3998798369727236| valid loss 0.47753847941108374\n",
      "Epoch:3|Train Loss:0.3998798369727236|Val Loss:0.47753847941108374\n",
      "Epoch:4|Train Loss:0.32059897878702653|Val Loss:0.48320841530094977\n",
      "Epoch:5|Train Loss:0.25451711478068473|Val Loss:0.5233797754930414\n",
      "Current lr: 0.001\n",
      "Epoch:6|Train Loss:0.20163947565758483|Val Loss:0.5911380389462346\n",
      "Epoch:7|Train Loss:0.16263481562441967|Val Loss:0.664529587911523\n",
      "Epoch:8|Train Loss:0.13550754026212591|Val Loss:0.7584762936053069\n",
      "Current lr: 0.0005\n",
      "Epoch:9|Train Loss:0.11727300979831118|Val Loss:0.7491331618765126\n",
      "Early stop!\n",
      "Start test model: lm-bll-power-75.0.pth\n",
      "Test Loss: 0.49435912396596826\n",
      "***负采样评率1.0***\n",
      "Save model path:lm-bll-power-100.0.pth| train loss 0.7036088384846424| valid loss 0.49228474756945734\n",
      "Epoch:1|Train Loss:0.7036088384846424|Val Loss:0.49228474756945734\n",
      "Save model path:lm-bll-power-100.0.pth| train loss 0.40138880385363357| valid loss 0.3998942906441896\n",
      "Epoch:2|Train Loss:0.40138880385363357|Val Loss:0.3998942906441896\n",
      "Save model path:lm-bll-power-100.0.pth| train loss 0.309843991189561| valid loss 0.37579628695612366\n",
      "Epoch:3|Train Loss:0.309843991189561|Val Loss:0.37579628695612366\n",
      "Epoch:4|Train Loss:0.24826236402100704|Val Loss:0.3795428638872893\n",
      "Epoch:5|Train Loss:0.19717377067563382|Val Loss:0.4078715845294621\n",
      "Current lr: 0.001\n",
      "Epoch:6|Train Loss:0.1561488555942444|Val Loss:0.456433971291003\n",
      "Epoch:7|Train Loss:0.12558954498393737|Val Loss:0.5160156294055607\n",
      "Epoch:8|Train Loss:0.1043568408473375|Val Loss:0.5865511699863102\n",
      "Current lr: 0.0005\n",
      "Epoch:9|Train Loss:0.09125367024953061|Val Loss:inf\n",
      "Early stop!\n",
      "Start test model: lm-bll-power-100.0.pth\n",
      "Test Loss: 0.39262326126513275\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_NUM = 20\n",
    "\n",
    "for p in range(1, 5):\n",
    "    BEST_VALID_LOSS = float('inf')\n",
    "    power = 0.25*p \n",
    "    print(\"***负采样评率{}***\".format(power))\n",
    "    model_name = 'lm-bll-power-{}.pth'.format(power*100)\n",
    "    word_set, word2idx, idx2word, word_freqs = create_word_set(train_file, dev_file, test_file, power=power)\n",
    "\n",
    "    # 设置 <pad> 值为 0\n",
    "    PAD_IDX = 0\n",
    "    idx2word[PAD_IDX] = '<pad>'\n",
    "    word2idx['<pad>'] = PAD_IDX\n",
    "    \n",
    "    train_sentences = load_corpus(train_file)\n",
    "    dev_sentences = load_corpus(dev_file)\n",
    "    test_sentences = load_corpus(test_file)\n",
    "\n",
    "    train_words = sentences2words(train_sentences)\n",
    "    dev_words = sentences2words(dev_sentences)\n",
    "    test_words = sentences2words(test_sentences)\n",
    "    train_data, train_label, train_neg = model_sequence(train_sentences, word2idx, word_freqs, sample_num=SAMPLE_NUM)\n",
    "    dev_data, dev_label, dev_neg = model_sequence(dev_sentences, word2idx, word_freqs, sample_num=SAMPLE_NUM)\n",
    "    test_data, test_label, test_neg = model_sequence(test_sentences, word2idx, word_freqs, sample_num=SAMPLE_NUM)\n",
    "    \n",
    "    \n",
    "    train_batch = gene_batch_data(train_data, train_label, train_neg, batch_size=BATCH_SIZE)\n",
    "    dev_batch = gene_batch_data(dev_data, dev_label, dev_neg, batch_size=BATCH_SIZE)\n",
    "    test_batch = gene_batch_data(test_data, test_label, test_neg, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    model = LSTMNegModel(EMBEDDING_DIM, EMBEDDING_OUT, HIDDEN_DIM, VOCAB_SIZE, SAMPLE_NUM)\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARN_RATE)  # 指定优化器\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)   # 学习率缩减？\n",
    "\n",
    "    SCHED_NUM = 0\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        train_loss = train(model, DEVICE, train_batch, optimizer, GRAD_CLIP)\n",
    "        valid_loss = evaluate(model, DEVICE, dev_batch)\n",
    "        if valid_loss < BEST_VALID_LOSS: # 如果是最好的模型就保存到文件夹\n",
    "            BEST_VALID_LOSS = valid_loss\n",
    "            torch.save(model, model_name)\n",
    "            print(\"Save model path:{}| train loss {}| valid loss {}\".format(model_name, train_loss, valid_loss))\n",
    "            SCHED_NUM = 0\n",
    "        else:\n",
    "            SCHED_NUM += 1\n",
    "            if SCHED_NUM % 3 == 0:\n",
    "                scheduler.step()\n",
    "                print(\"Current lr:\", optimizer.param_groups[0]['lr'])\n",
    "            if SCHED_NUM == 7:\n",
    "                print(\"Early stop!\")\n",
    "                break\n",
    "        print('Epoch:{}|Train Loss:{}|Val Loss:{}'.format(epoch, train_loss, valid_loss))\n",
    "    print(\"Start test model:\", model_name)\n",
    "    model = torch.load(model_name)\n",
    "    test_loss = evaluate(model, DEVICE, test_batch)\n",
    "    print('Test Loss: {}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
