{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言模型\n",
    "------\n",
    "## 1 简介\n",
    "- pytorch实现LSTM训练语言模型\n",
    "- 使用的数据集：\n",
    "    - bobsue.lm.train.txt：语言模型训练数据（LMTRAIN）\n",
    "    - bobsue.lm.dev.txt：语言模型验证数据（LMDEV）\n",
    "    - bobsue.lm.test.txt：语言模型测试数据（LMTEST）\n",
    "    - bobsue.prevsent.train.tsv：基于上文的语言模型训练数据（PREVSENTTRAIN）\n",
    "    - bobsue.prevsent.dev.tsv：基于上文的上文语言模型验证数据（PREVSENTDEV）\n",
    "    - bobsue.prevsent.test.tsv：基于上文的上文语言模型测试数据（PREVSENTTEST）\n",
    "    - bobsue.voc.txt：词汇表文件，每行是一个单词\n",
    "    - lm文件中的每一行都包含一个故事中的句子。prev文件中的每一行都包含一故事中的一个句子，tab，然后是故事中的下一个句子。注意：prevsent文件中每一行的第二个字段与相应的lm文件中的对应行相同。( 也就是说：cut -f 2 bobsue.prevsent.x.tsv与bobsue.lm.x.txt相同）完整的词汇表包含在文件bobsue.voc.txt中，每一行是一个单词。在这个任务中不会出现未知单词。\n",
    "- 评估\n",
    "    - 我们使用单词预测准确率作为主要评估指标而非困惑度(perplexity)。因为当你试图比较某些损失函数时，perplexity不太好用。\n",
    "\n",
    "-----\n",
    "## 2 具体内容\n",
    "### 用Log Loss训练LSTM模型（30分）\n",
    "- 首先，请实现一个基于LSTM的语言模型。也就是说，对每个当前的hidden state做一个线性变化和softmax处理，预测下一个单词。使用Log Loss (Cross Entropy Loss)来训练该模型。使用EVALLM的步骤来评估模型。汇报你的模型训练结果和代码。你的单词预测准确率应该能够达到30%以上。\n",
    "\n",
    "    - 至少训练10个epoch\n",
    "    - 你可以使用不同的模型参数。我们建议你使用一层LSTM，200 hidden dimension作为词向量和LSTM hidden state的大小。\n",
    "    - 模型参数的初始值可以随机设定\n",
    "    - 输入和输出层的word embedding参数可以不一样，当然你也可以尝试把他们设置成一样。\n",
    "    - 使用Adam或者SGD等optimizer来优化模型。\n",
    "    - 在提交报告的时候请尽可能详细描述你的所有模型参数。\n",
    "----\n",
    "### 错误分析（15分）\n",
    "- 请在你的代码中添加一项功能，可以展示出你模型预测错误的单词，将标准答案单词和模型预测的单词分别打印出来。（5分）\n",
    "- 请写下你的模型最常见的35个预测错误(正确答案是a，模型预测了b)。（5分）\n",
    "- 通过观察这些常见的错误，将错误分类。你不需要将每个错误都分类，不过建议同学们花点时间观察自己模型的错误，看看他们是否有一定的相关性。大家可以尝试从以下角度思考错误类型：为什么你的模型会预测出这个单词？模型怎么样才能做得更好？这个模型犯的错误是很接近正确答案的吗？如果是的话，这个错误答案与正确答案有何相似之处？把这35个预测错误归类成你定义的错误类别。讨论一下你的模型在哪些方面做得比较好，哪些方面做的不好。（5分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据文件\n",
    "word_file = './data/bobsue.voc.txt'\n",
    "train_file = './data/bobsue.lm.train.txt'\n",
    "test_file = './data/bobsue.lm.test.txt'\n",
    "dev_file = './data/bobsue.lm.dev.txt'\n",
    "\n",
    "BATCH_SIZE = 32       # 批次大小\n",
    "EMBEDDING_DIM = 200   # 词向量维度\n",
    "HIDDEN_DIM = 200      # 隐含层\n",
    "GRAD_CLIP = 5.        # 梯度截断值\n",
    "EPOCHS = 20 \n",
    "LEARN_RATE = 0.01     # 初始学习率\n",
    "\n",
    "BEST_VALID_LOSS = float('inf')     # 初始验证集上的损失值，设为最大\n",
    "MODEL_PATH = \"lm-best-dim{}.pth\"   # 模型名称\n",
    "USE_CUDA = torch.cuda.is_available()    # 是否使用GPU\n",
    "NUM_CUDA = torch.cuda.device_count()    # GPU数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 数据预处理\n",
    "1.1 读取数据文件，构建词汇集、word2idx、idx2word。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_set(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        word_set = set([line.strip() for line in f])\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = load_word_set(word_file)\n",
    "word2idx = {w:i for i, w in enumerate(word_set, 1)}\n",
    "idx2word = {i:w for i, w in enumerate(word_set, 1)}\n",
    "PAD_IDX = 0\n",
    "word2idx[\"<pad>\"] = PAD_IDX\n",
    "idx2word[PAD_IDX] = \"<pad>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练验证测试数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(filename):\n",
    "    \"\"\"读取数据集，返回句子列表\"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = [line.strip() for line in f]\n",
    "    return sentences\n",
    "\n",
    "def sentences2words(sentences):\n",
    "    return [w for s in sentences for w in s.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = load_corpus(train_file)\n",
    "dev_sentences = load_corpus(dev_file)\n",
    "test_sentences = load_corpus(test_file)\n",
    "\n",
    "train_words = sentences2words(train_sentences)\n",
    "dev_words = sentences2words(dev_sentences)\n",
    "test_words = sentences2words(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数: 6036，单词数: 71367.\n",
      "验证集句子数: 750，单词数: 8707.\n",
      "测试集句子数: 750，单词数: 8809.\n"
     ]
    }
   ],
   "source": [
    "s = \"{}句子数: {}，单词数: {}.\"\n",
    "print(s.format(\"训练集\", len(train_sentences), len(train_words)))\n",
    "print(s.format(\"验证集\", len(dev_sentences), len(dev_words)))\n",
    "print(s.format(\"测试集\", len(test_sentences), len(test_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sentence_num(sentences):\n",
    "    \"\"\"返回最长句子单词数量\"\"\"\n",
    "    return max([len(s.split()) for s in sentences ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集最长句子单词个数： 21\n",
      "验证集最长句子单词个数： 20\n",
      "测试集最长句子单词个数： 21\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集最长句子单词个数：\", max_sentence_num(train_sentences))\n",
    "print(\"验证集最长句子单词个数：\", max_sentence_num(dev_sentences))\n",
    "print(\"测试集最长句子单词个数：\", max_sentence_num(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sequence(corpus, word2idx, seq_len=21):\n",
    "    \"\"\"语料句子转换成模型输入的序列idx\"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        sentence_vec = [0]*seq_len\n",
    "        for i, w in enumerate(words[:-1]):\n",
    "            sentence_vec[i] = word2idx[w]\n",
    "        sentences.append(sentence_vec)\n",
    "        label_vec = [0] * seq_len\n",
    "        for i, w in enumerate(words[1:]):\n",
    "            label_vec[i] = word2idx[w]\n",
    "        labels.append(label_vec)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label = model_sequence(train_sentences, word2idx)\n",
    "dev_data, dev_label = model_sequence(dev_sentences, word2idx)\n",
    "test_data, test_label = model_sequence(test_sentences, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[978, 1003, 1423, 106, 1219, 835, 1240, 143, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1003, 1423, 106, 1219, 835, 1240, 143, 603, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[1], train_label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> The girl broke up with Bob . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "The girl broke up with Bob . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> "
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "for i in train_data[idx]:\n",
    "    print(idx2word[i],  end=' ')\n",
    "print('\\n')\n",
    "for i in train_label[idx]:\n",
    "    print(idx2word[i], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_data\n",
    "def gen_batch_data(data, label, batch_size=32):\n",
    "    \"\"\"构建 batch tensor，返回 batch 列表，每个batch为二元组包含data和label\"\"\"\n",
    "    batch_data = []\n",
    "    data_tensor = torch.tensor(data, dtype=torch.long)\n",
    "    label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "    n, dim = data_tensor.size()\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = start + batch_size\n",
    "        if end > n:\n",
    "            dbatch = data_tensor[start: ]\n",
    "            lbatch = label_tensor[start: ]\n",
    "            print(\"最后一个batch size:\", data_tensor.size())\n",
    "            break\n",
    "        else:\n",
    "            dbatch = data_tensor[start: end]\n",
    "            lbatch = label_tensor[start: end]\n",
    "        batch_data.append((dbatch, lbatch))\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = gen_batch_data(train_data, train_label, batch_size=BATCH_SIZE)\n",
    "dev_batch = gen_batch_data(dev_data, dev_label, batch_size=BATCH_SIZE)\n",
    "test_batch = gen_batch_data(test_data, test_label, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 23 23\n"
     ]
    }
   ],
   "source": [
    "print(len(train_batch), len(dev_batch), len(test_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.n_word = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(self.n_word, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2word = nn.Linear(hidden_dim, self.n_word)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.word_embeddings(x)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embeds)\n",
    "        target_space = self.hidden2word(lstm_out.contiguous().view(-1, self.hidden_dim))\n",
    "        mask = (x != PAD_IDX).view(-1)\n",
    "        pure_target = target_space[mask]\n",
    "        \n",
    "        target_scores = F.log_softmax(pure_target, dim=1)\n",
    "        return target_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2idx)\n",
    "model = MyLSTM(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用多块GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else 'cpu')\n",
    "model = model.to(DEVICE)\n",
    "if NUM_CUDA > 1:\n",
    "    device_ids = list(range(NUM_CUDA))\n",
    "    print(device_ids)\n",
    "    model = nn.DataParallel(model, device_ids=device_ids)\n",
    "    # model = nn.parallel.DistributedDataParallel(model, device_ids=device_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_score(y_hat, y):\n",
    "    pred = y_hat.argmax(dim=1)\n",
    "    # print(y.view(-1))\n",
    "    acc_count = torch.eq(pred, y.view(-1))\n",
    "    score = acc_count.sum().item() / acc_count.size()[0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, iterator, optimizer, criterion, grad_clip):\n",
    "    epoch_loss = 0  # 积累变量\n",
    "    epoch_acc = 0   # 积累变量\n",
    "    model.train()   # 该函数表示PHASE=Train\n",
    "    \n",
    "    for x, y in iterator:  # 拿每一个minibatch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mask = y != PAD_IDX\n",
    "        pure_y = y[mask]\n",
    "        \n",
    "        fx = model(x)                 # 进行forward\n",
    "        loss = criterion(fx, pure_y)  # 计算loss\n",
    "        acc = acc_score(fx, pure_y)   # 计算准确率\n",
    "        loss.backward()               # 进行BP\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()  # 更新参数\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss/len(iterator),epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, iterator, criterion):\n",
    "    model.eval()  # 不更新参数，预测模式\n",
    "    epoch_loss=0  # 积累变量\n",
    "    epoch_acc=0   # 积累变量\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in iterator:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            mask = y != PAD_IDX\n",
    "            pure_y = y[mask]\n",
    "            \n",
    "            fx = model(x)\n",
    "            loss = criterion(fx, pure_y)\n",
    "            acc = acc_score(fx, pure_y)\n",
    "            epoch_loss += loss\n",
    "            epoch_acc += acc\n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current lr: 0.01\n",
      "Epoch:1|Train Loss:3.1130173206329346|Train Acc:0.3461452941338287|Val Loss:3.5606744289398193|Val Acc:0.32080264668716474\n",
      "Current lr: 0.005\n",
      "Epoch:2|Train Loss:2.7067654132843018|Train Acc:0.37787717063887194|Val Loss:3.6790380477905273|Val Acc:0.31614916318423003\n",
      "Epoch:3|Train Loss:2.2615437507629395|Train Acc:0.4525100845817822|Val Loss:3.7166340351104736|Val Acc:0.3220625274023484\n",
      "Epoch:4|Train Loss:2.0058023929595947|Train Acc:0.5062772957559821|Val Loss:3.810671329498291|Val Acc:0.318610500632823\n",
      "Epoch:5|Train Loss:1.8320494890213013|Train Acc:0.5477256376905681|Val Loss:3.9121179580688477|Val Acc:0.31444531440209145\n",
      "Epoch:6|Train Loss:1.6956899166107178|Train Acc:0.5821775704339199|Val Loss:4.022996425628662|Val Acc:0.3168135065121687\n",
      "Early stop!\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()            # 指定损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARN_RATE)  # 指定优化器\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)   # 学习率缩减？\n",
    "\n",
    "SCHED_NUM = 0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss, train_acc = train(model, DEVICE, train_batch, optimizer, criterion, GRAD_CLIP)\n",
    "    valid_loss, valid_acc = evaluate(model, DEVICE, dev_batch, criterion)\n",
    "    if valid_loss < BEST_VALID_LOSS: # 如果是最好的模型就保存到文件夹\n",
    "        BEST_VALID_LOSS = valid_loss\n",
    "        torch.save(model, MODEL_PATH.format(EMBEDDING_DIM))\n",
    "        SCHED_NUM = 0\n",
    "    else:\n",
    "        SCHED_NUM += 1\n",
    "        if SCHED_NUM // 3 == 0:\n",
    "            scheduler.step()\n",
    "            print(\"Current lr:\", optimizer.param_groups[0]['lr'])\n",
    "        if SCHED_NUM == 7:\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "    print('Epoch:{0}|Train Loss:{1}|Train Acc:{2}|Val Loss:{3}|Val Acc:{4}'.format(epoch,train_loss,train_acc,valid_loss,valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.536245346069336 | Test Acc: 0.3199952130980169 |\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(MODEL_PATH.format(EMBEDDING_DIM))\n",
    "test_loss, test_acc = evaluate(model, DEVICE, test_batch, criterion)\n",
    "print('| Test Loss: {0} | Test Acc: {1} |'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 打印错误单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred_error_words(model,device,data_batch):\n",
    "    model.eval()\n",
    "    error_words = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_batch:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            mask = (y!=PAD_IDX)\n",
    "            fx = model(x)\n",
    "            \n",
    "            pred_idx = fx.argmax(dim=1)\n",
    "            ground_truth_idx = y[mask]\n",
    "            for p, g in zip(pred_idx.tolist(), ground_truth_idx.tolist()):\n",
    "                if p != g:\n",
    "                    error_words.append(\" | \".join([idx2word[g], idx2word[p]]))\n",
    "    return error_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(MODEL_PATH.format(EMBEDDING_DIM))\n",
    "error_words = print_pred_error_words(model, DEVICE, test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真实值 | 预测值 | 预测错误次数\n",
      "('Bob | He', 137)\n",
      "('She | He', 109)\n",
      "('Sue | He', 89)\n",
      "('to | .', 54)\n",
      "('and | .', 46)\n",
      "('her | the', 42)\n",
      "('decided | was', 42)\n",
      "('had | was', 42)\n",
      "('his | the', 37)\n",
      "('a | the', 35)\n",
      "('for | .', 34)\n",
      "(', | .', 31)\n",
      "('in | .', 29)\n",
      "('His | He', 26)\n",
      "('One | He', 25)\n",
      "('. | to', 24)\n",
      "('got | was', 21)\n",
      "('But | He', 21)\n",
      "('Her | He', 21)\n",
      "('The | He', 21)\n",
      "('she | he', 20)\n",
      "(\"'s | was\", 19)\n",
      "('go | get', 19)\n",
      "('When | He', 19)\n",
      "('They | He', 19)\n",
      "('he | Bob', 17)\n",
      "('. | the', 17)\n",
      "('went | was', 17)\n",
      "('her | a', 16)\n",
      "('wanted | was', 16)\n",
      "('it | the', 15)\n",
      "('at | .', 15)\n",
      "('! | .', 15)\n",
      "('the | .', 15)\n",
      "('he | to', 15)\n"
     ]
    }
   ],
   "source": [
    "words_counter = Counter(error_words)\n",
    "TopN = 35\n",
    "topn_words = words_counter.most_common(TopN)\n",
    "print(\"真实值 | 预测值 | 预测错误次数\")\n",
    "for w in topn_words:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
